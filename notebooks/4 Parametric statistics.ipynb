{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca898a89-1d92-4fa2-b355-c286e9961408",
   "metadata": {},
   "source": [
    "# Parametric statistics\n",
    "\n",
    "The `statsmodels` and `scipy` packages provide functions for parametric statistics (i.e. distribution based). `scipy` provides many of the underlying statiscal methods, while `statsmodels` provides the ability to run regression models. `numpy` provides functions for manipulating multidimensional arrays and matrices.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "As usual, we import `pandas`. We will also import `numpy` as `np`, `statsmodels.api` as `sm`. Scipy is frequently not aliased with a shorter name, but this is a matter of personal preferenceâ€”there is no accepted standard like there is with `pandas`, `numpy`, and `statsmodels`. We import `scipy.stats` with no alias. We also import `statsmodels.stats`, which contains some utility functions, with no alias.\n",
    "\n",
    "We are also going to import `matplotlib` to make some diagnostic plots for our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57986dea-27d5-413c-ac83-8a2a418df797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.outliers_influence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e75f30-0c80-490c-9269-918263fcc9e1",
   "metadata": {},
   "source": [
    "## Reading data from Excel\n",
    "\n",
    "Next, we need to read in the data. We are using data on residential property assessments and sales from the [Wake County Assessor](https://www.wakegov.com/departments-government/tax-administration/data-files-statistics-and-reports/real-estate-property-data-files). I have already filtered the data file to only properties that sold since 2020 to reduce the file size.\n",
    "\n",
    "The data are in Excel format. We can read Excel data directly into Python using the `pd.read_excel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b267f-e935-4f8b-a7ca-7641841c0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/recent_sfh_sales.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0313efb-d020-4c58-a0bf-3e3689c99ef8",
   "metadata": {},
   "source": [
    "## Learning more information about the dataset\n",
    "\n",
    "This dataset is large, and we can't see the whole thing at once. One potentially helpful thing to do is to print out the names of all of the columns. The names can be accessed with `data.columns` or `data.columns.values` - the second form will not truncate the list if it is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9a6dc-51a7-4e83-88ed-b25f8739e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e538dd7-a841-4b06-9aba-848135051c8f",
   "metadata": {},
   "source": [
    "## Are tax assessments accurate?\n",
    "\n",
    "The data include information both on sale cost (`Total_sale_Price`) and assessed value (`Assesed_Building_Value` and `Assessed_Land_Value`). In theory, these should track fairly closely, but in practice they may not. Let's first make a scatter plot plotting sale price vs. assessed value. We will first need to create a new variable that contains total assessed value (land plus building)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833e40f-3a34-43e6-b47f-fc8d0b72cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Assessed_Total_Value\"] = data.Assessed_Building_Value + data.Assessed_Land_Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80401ba-afbd-480f-a029-71b528205bc8",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make a scatter plot with sale price on the x axis and building value on the y axis. Label the axes. Only display the portion of the chart with sale and assessed values below $2 million. Add a 45 degree line to your plot with\n",
    "\n",
    "`plt.plot([0, 2_000_000], [0, 2_000_000])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e930f35-d423-4e35-938e-9a6784fe00c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab888131-b307-457d-b7aa-ad70f2c573be",
   "metadata": {},
   "source": [
    "### Testing the relationship between assessed and sale value\n",
    "\n",
    "It looks like sale prices tend to be higher than assessed values. We can compute the mean difference between the two using the `np.mean` function.\n",
    "\n",
    "Many functions are available in both Pandas and Numpy, and can often be used interchangeably.\n",
    "\n",
    "I also added assert statements to make sure there are no missing values before we proceed with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c0460-400d-417a-8e64-97334915b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data.Total_sale_Price - data.Assessed_Total_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b15d7a-6db3-4e10-89bc-7ebee0d52597",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not data.Total_sale_Price.isnull().any()\n",
    "assert not data.Assessed_Total_Value.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01691f6f-3441-49ed-83ed-0ccc967f83b3",
   "metadata": {},
   "source": [
    "#### Evaluating statistical significance\n",
    "\n",
    "We see that the average home sold for $87,000 more than it was assessed for. We can check whether this is statistically significant with a paired-sample _t_-test. This is available in the `scipy.stats.ttest_rel` function (for a _t_-test of related samples; `ttest_ind` is also available for independent samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7473c48-36e9-4ef4-a8b3-7fb97ea34c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(data.Total_sale_Price, data.Assessed_Total_Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3427af9-2dac-46d1-b833-db02ee0bad18",
   "metadata": {},
   "source": [
    "Scipy also has functionality for working with distributions, so if you need to construct your own test you can. Below, we do this with a paired-sample t-test. I wouldn't recommend creating your own code for tests that are available in Scipy, but this demonstrates how you would do it if you were using a test that was not available in Scipy (for instance, if you did not have data on the two values, but only on the difference between them, and thus could not use `ttest_rel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dced031-ab59-4078-8611-56e147f46680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, compute the test statistic. This is the average difference, divided by the standard error.\n",
    "# the standard error is the standard deviation of the differences divided by the square root of the sample size.\n",
    "avg_diff = np.mean(data.Total_sale_Price - data.Assessed_Total_Value)\n",
    "std_err = np.std(data.Total_sale_Price - data.Assessed_Total_Value) / np.sqrt(len(data))\n",
    "test_stat = avg_diff / std_err\n",
    "test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71770-1ac6-44da-95fd-955733a59735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we can use the t cumulative distribution function to find the p-value\n",
    "# We use abs to ensure the test statistic is positive, compute the cumulative distribution function\n",
    "# with the correct number of degrees of freedom, and subtract from 1 to get the tail\n",
    "p_value = 1 - scipy.stats.t.cdf(np.abs(test_stat), len(data))\n",
    "\n",
    "# we then multiply the p_value by 2 for a two-tailed test. The *= operator multiplies the left hand side by\n",
    "# the right hand, and saves the result into the variable on the left hand side\n",
    "p_value *= 2\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae23a4-7fb6-4422-b9a3-90b354ee456c",
   "metadata": {},
   "source": [
    "## Regression models\n",
    "\n",
    "A very common form of inferential statistics is the regression model. `statsmodels` provides facilities to estimate many types of regression model; here, we'll estimate a linear regression model of home price.\n",
    "\n",
    "If you're not familiar with linear regression, it's just a way to estimate an equation that best expresses an outcome in terms of other variables.\n",
    "\n",
    "We will create a regression of home price on year built, acreage, heated area, city, and utilities available. First, we need to put the data in the right format.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The `UTILITIES` column contains which utilities are available at the property. It is a code that contains \"W\" if water is available, \"S\" is if sewer is available, \"E\" if electricity is available, \"G\" if gas is available, and \"ALL\" if all are available. For the regression model, we want to parse this out into four variables `water`, `sewer`, `gas`, and `electric`, one for each utility. We will use the string accessors we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019fac3-056e-4add-a9ef-ab285b40dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b43a6e-8346-4697-82cc-5981a4757084",
   "metadata": {},
   "source": [
    "#### Check your work\n",
    "\n",
    "To make sure this worked, let's print out the proportion of houses with each utility available. Almost every house should have electric available, while less will have water, gas, and sewer. We would expect fewer houses to have sewer than water, as houses with sewer system connections will almost always also have city water, while many homes have city water but rely on septic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf0905-4613-41a8-b297-ec7605ae5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.electric.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf3b77-f262-4ba5-a0f9-e4e302868a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.water.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be94dee-d73f-4b23-b8ea-b1644ca84fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452e427-b28a-4a83-a1dd-adeb7774d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sewer.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298eae74-2489-4ff5-add9-2ea23fc0db62",
   "metadata": {},
   "source": [
    "### Extracting variables for the regression\n",
    "\n",
    "Next, we want to extract all of the variables for the regression. We will first create a data frame that has just the variables we care about. I am leaving out electric since it is true for almost all of the properties. I am also removing properties that sold for less than $15,000 as these are probably bad data or otherwise unrepresentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98563d9d-9645-40a9-8cb6-50d8a83e6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = data.loc[data.Total_sale_Price > 15_000, [\"Total_sale_Price\", \"Year_Built\", \"Deeded_Acreage\", \"HEATED_AREA\", \"CITY\",\n",
    "                 \"water\", \"gas\", \"sewer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e850c3d-b9f1-470f-9ec0-a230bf29fcfc",
   "metadata": {},
   "source": [
    "### Check for null/missing values\n",
    "\n",
    "Next, we want to check to see if there are any missing values in our regression variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8f244-c292-41f1-8cb5-131bb04eb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6602a8-cf4d-4be1-bc22-88d78bce30ef",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There are some null values in the `CITY` variable. Look at some of these records, and look their addresses up in Google Maps (note: the address is split across multiple columns). Do you have an idea why these might be missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9fa6d-0794-4e23-83bb-0171c6d3ccca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7ea28b-7371-4ca4-901d-7053572bfa20",
   "metadata": {},
   "source": [
    "### Replace missing values\n",
    "\n",
    "We don't actuall want to drop these records. Instead, we'll replace the missing values with a value indicating why they are missing. (Fill in the value below. I didn't fill it in so I wouldn't give away the answer above!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c6fa9-93ab-41e9-b544-6bfc208b9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.CITY.isnull(), \"CITY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a874c-343a-4c8f-b031-ad21f86a2a5e",
   "metadata": {},
   "source": [
    "## Running the regression\n",
    "\n",
    "`statsmodels` regression interface is a little bit more low-level than other statistical software you may have used. In particular, it will not automatically create dummy variables for non-numeric columns, and does not automatically add a constant - we have to do both of those things manually. Fortunately, there are functions available to help us.\n",
    "\n",
    "`pd.get_dummies` will convert columns into dummy variables. `sm.add_constant` adds a constant column.\n",
    "\n",
    "First, we'll add the dummy variables. By default `get_dummies` will convert all object and category columns, but I prefer to manually specify the columns I want dummies for, to ensure I get what I am intending to. For most types of regression you need to drop one of the dummy variables. Pandas can do this automatically by include `drop_first=True` in your call to `get_dummies`, but then you don't have any control over which category is the base category. Here, we will make Raleigh the base category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5af86-72fb-443c-9b24-4367e3a4bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = pd.get_dummies(reg_data, columns=[\"CITY\"]).drop(columns=[\"CITY_RAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f952d-ad5c-40fc-8510-04f81a2cdbb3",
   "metadata": {},
   "source": [
    "Now, we are ready to run the regression. `sm.add_constant` will return a new data frame, but since the constant column is not interesting it's typical to just include the call to `sm.add_constant` when running the regression. We also drop the total sale price column from the right hand side of the regression.\n",
    "\n",
    "Running a regression in `statsmodels` is a two-step process. First, we create the regression model, then we use the `.fit()` function to estimate it.\n",
    "\n",
    "I've divided the dependent variable by 1000, to make the results easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5792151-dff3-437e-a418-9a8b85fb9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(\n",
    "    reg_data.Total_sale_Price / 1000, # dependent variable, referred to as \"endog\" in statsmodels documentation\n",
    "    sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\"))\n",
    ")\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f56264-ac61-4b66-b731-af74fa5e281f",
   "metadata": {},
   "source": [
    "Running the code above gave us a common error message when running a regression; `statsmodels` complained that our data had type \"object\", which is non-numeric and cannot be used in a regression model. We can look at the dtypes to see if we can figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade5c8c-58cd-4f68-af66-3f7612990a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\")).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef8d33-b670-4e7a-a4fc-e771dcc3d0e7",
   "metadata": {},
   "source": [
    "These all are numeric types (`bool` is True/False, but could also be considered 0/1). The issue is that `statsmodels` needs all of the columns to be the _same_ type, because it uses `numpy` internally; numpy matrices have a single type. This greatly increases speed, at the cost of some flexibility. The error message gave us a hint - we can use `np.asarray(dataframe)` to see what type it is choosing for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2647c5a-5181-4da0-9af5-3db58586e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a787f6-6940-469a-85c3-9e14fc971015",
   "metadata": {},
   "source": [
    "Well, there's the problem - it's getting converted to a matrix of \"object\". We can explicitly tell it that we want everything converted to a real number (`float64`) when we run the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255c1db-8395-48fd-88e6-4f684a08403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(\n",
    "    reg_data.Total_sale_Price / 1000, # dependent variable, referred to as \"endog\" in statsmodels documentation\n",
    "    sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\")).astype(\"float64\")\n",
    ")\n",
    "fit = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503cc69b-1bce-49f0-9a0d-647540f1211a",
   "metadata": {},
   "source": [
    "We didn't get any errors, so the model ran. We can use the `fit.summary()` method to see a regression table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412f501-8ed3-4965-9ed4-f694abaf2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe96476-c073-4ffc-952a-1008da9360de",
   "metadata": {},
   "source": [
    "### Regression diagnostics\n",
    "\n",
    "It's a good idea to run some regression diagnostics after a regression. Statsmodels is again a bit lower-level than many statistical packages; many of the standard regression diagnostics need to be implemented manually. We see a note about multicollinearity in the results above, so we may want to check Variance Inflation Factors.\n",
    "\n",
    "The `statsmodels` variance inflation factor function checks one column at a time, and requires the column number and a data frame with all of the columns used in estimation. Typically, we want to look all VIFs for all columns at once. We can use a loop to do this.\n",
    "\n",
    "Loops let you write some code that will be executed many times. They are often discouraged in Python, because looping over many values can be slow (for instance, looping over all the rows of a data frame). However, here we are only looping over a few dozen columns, so performance should not be an issue.\n",
    "\n",
    "We are using a for loop here; for loops run the code within them once for each value in a list, array, or other object that supports _iteration_. Here, we are iterating over the column names of the data, and using the `enumerate` function to get the column numbers. The code within a for loop is anything that is indented below line with \"for\" on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b627f41-2103-4db7-bee8-04dc53dbb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a variable with exactly the data used in regression\n",
    "exog = sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\")).astype(\"float64\")\n",
    "\n",
    "for column_index, column_name in enumerate(exog.columns):\n",
    "    vif = statsmodels.stats.outliers_influence.variance_inflation_factor(exog, column_index)\n",
    "    print(column_name, vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0e499-ba98-401b-889d-616f9f3da777",
   "metadata": {},
   "source": [
    "### Residuals vs fitted plot\n",
    "\n",
    "Another common plot is the residuals vs. fitted plot, which can inform you of heteroskedasticity. The residuals and fitted values are available within the fit object.\n",
    "\n",
    "There are other regression plots as well, but the point of this class is not to be an introduction to regression - this is just to show you how you might construct some of these plots yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df331b1b-dfcc-4513-9eca-e753c31e6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(fit.fittedvalues, fit.resid, s=0.1)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af2501-7e04-4e61-94d6-310f3ac055f8",
   "metadata": {},
   "source": [
    "The characteristic shape suggests heteroskedasticity. We can address that by taking the logarithm of the dependent variable. We can use the `np.log` function to transform the variable.\n",
    "\n",
    "We'll also transform the year built variable. We might expect that there is a nonlinear relationship between year built and value, so we'll add a squared term. To use a squared (or other transformed) term in statsmodels, we just add the transformed column to the data frame. Interactions would be similar - create a new column multiplying existing columns together.\n",
    "\n",
    "`**` is the power operator in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b00cf1-c248-4819-8629-ad6586df8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data[\"Year_Built_Squared\"] = reg_data.Year_Built ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0b4b1-f8db-4a50-ac79-26baa7470583",
   "metadata": {},
   "source": [
    "Now that there is another column in the `reg_data` data frame, we can just re-run the same regression code to include the new column. We use `np.log` to transform the sale price before running the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d90c33-e977-491a-9af2-7ec19aff4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(\n",
    "    np.log(reg_data.Total_sale_Price / 1000), # dependent variable, referred to as \"endog\" in statsmodels documentation\n",
    "    sm.add_constant(reg_data.drop(columns=\"Total_sale_Price\")).astype(\"float64\")\n",
    ")\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729afa27-05c8-497c-941b-c95f136ac17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
