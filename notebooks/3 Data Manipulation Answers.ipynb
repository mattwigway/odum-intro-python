{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf7bd76-c736-45d4-a152-74328ce3d321",
   "metadata": {},
   "source": [
    "# Data manipulation and cleaning\n",
    "\n",
    "In this exercise, we will cover more advanced data manipulation techniques available in the Pandas library, particularly techniques for cleaning up messy data. We will learn to address missing data, recode data columns, and modify data based on conditions. For this exercise, we'll be using the Wake County restaurant available [from the Wake County open data portal](https://data-wake.opendata.arcgis.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac881e-9e64-4790-8ace-19d14b2f7c00",
   "metadata": {},
   "source": [
    "## Exercise: import libraries\n",
    "\n",
    "As before, we need to import the `pandas` library. Go ahead and import that library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd065ab-d6ef-419e-b386-408ccd14610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6735fb-450b-4139-9572-bc4d2d50583c",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "There are two data files, one with general restaurant information, and one with inspection results. We need to read both, join them into a single dataset, and do some cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126454-dab0-47ac-85b7-89b0c092a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv(\"https://raw.githubusercontent.com/mattwigway/odum-intro-python/main/data/restaurants.csv\")\n",
    "inspections = pd.read_csv(\"https://raw.githubusercontent.com/mattwigway/odum-intro-python/main/data/restaurant_inspections.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a826305-275c-42f1-8754-d137d50244c7",
   "metadata": {},
   "source": [
    "## Exercise: Understanding the data\n",
    "\n",
    "The first step in using a dataset like this is to understand the data. We want to put these datasets together into a single dataset, with one row per restaurant with information on its most recent inspection. A few questions to ask ourselves when looking at this dataset:\n",
    "- What common columns are there that might allow us to figure out which row in one table is associated with which row in the other table?\n",
    "- Do both datasets have a single record for each restaurant?\n",
    "\n",
    "See if you can write Python code to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523d823-593d-422b-815c-7148cf90e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041492e-39ff-4a05-9c04-fdd9d7b414cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480f60e-3ef9-491b-9ed5-53e3287ccb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use .value_counts() to see how many inspection records there are per restaurant\n",
    "inspections.HSISID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8beec-f36c-4c75-869e-25660fb7f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's clear that there is more than one inspection per restaurant, just from the code above.\n",
    "# But suppose that the restaurants near the start and end of the dataset had one inspection each, but some in the\n",
    "# middle had multiple - that wouldn't be clear from the preview above. We can look at the .max() function of\n",
    "# .value_counts() to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff219e-2931-478d-804c-ab1272a0b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.HSISID.value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830e5ad-6176-4882-8253-adc03fcaa4d6",
   "metadata": {},
   "source": [
    "## Finding the most recent inspection of each restaurant\n",
    "\n",
    "We want one record per restaurant in the inspections dataset, and we want that record to have the most recent inspection. We will first convert the date column to a date type. Then, we will sort the data frame by restaurant ID and date descending, so the most recent inspection is always the first one. Then, we will use a new function of `groupby()`, `nth()`, which returns a new data frame with the n-th row of each group.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Convert the date column to a date type. The codes for specifying date formats [are found here](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c5e8-0f11-4f32-8dc9-5f7305691666",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections[\"DATE_\"] = pd.to_datetime(inspections.DATE_, format=\"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d048e74-5bd6-4032-969b-8ef8a264a306",
   "metadata": {},
   "source": [
    "### Sort and group by\n",
    "\n",
    "We sort the inspections by restaurant ID and date in-place, and then get the first record from each restaurant. Python uses zero-based indexing, so the first element is element 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4174d-d2bd-4a59-9832-5ba1fdd0ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.sort_values([\"HSISID\", \"DATE_\"], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979cd9a-235e-4598-94e4-022c40a6612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_inspection = inspections.groupby(\"HSISID\").nth(0)\n",
    "latest_inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d62228-e31d-4229-ad36-b8028331238b",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "A \"join\" is a way of putting multiple datasets together when they share a column with a common _key_ (HSISID in this case). There are several types of joins: left joins, right joins, inner joins, and outer joins. They all join two datasets, which are generally referred to as \"left\" and \"right\". The result of a join is a new, combined dataset with the columns from both of the original datasets. The types of joins differ in how they handle rows that do not match between the datasets:\n",
    "\n",
    "- Left join: preserve all rows in the left dataset, drop rows from right dataset that do not match, and leave columns from right dataset empty when there is a left row with no corresponding right row\n",
    "- Right join: preserve all rows in the right dataset, drop rows from the left that do not match, and leave columns from left dataset empty when there is no corresponding right row\n",
    "- Inner join: only preserve rows that match between the two datasets\n",
    "- Outer join: preserve all rows \n",
    "\n",
    "Additionally, there are differences in the _number_ of rows that match. A one-to-one join is the simplest, when there is one record for each key in both datasets. Many-to-one and one-to-many joins have duplicated keys in the left or right datasets, respectively, while a many-to-many join has duplicate keys in both datasets. When there are duplicated keys in either dataset, the corresponding rows from the other dataset will be duplicated to match. When there are duplicated keys in _both_ datasets, rows are duplicated so that there is one row with each _combination_ of matching rows - so if there are two rows with the same key in both datasets, there will be four rows in the resulting dataset. We can \"validate\" our joins to make sure that the rows we think should be unique actually are. If we don't do this, it can lead to misleading results if some records are inadvertently duplicated during the join process.\n",
    "\n",
    "The default in Pandas is to perform an inner join. Personally, I always prefer to perform a left or outer join, and then drop records that didn't match in a separate step, to make sure that I understand what I'm dropping and don't inadvertently drop records that I didn't mean to. Pandas has the option to create an \"indicator\" during a join that provides more information about the join.\n",
    "\n",
    "Confusingly, the function to join two datasets in pandas is called `merge`. There is also a `join` function, which is similar but must work with the index rather than a column of the right dataset, whereas `merge` can use either a column or the index. I recommend always using `merge` rather than `join`. Here we merge on the HSISID in the restaurants dataset. The HSISID column has become the index in latest_inspections, so we either need to specify that we're joining on the index in the right dataset, or reset the index to convert HSISID back to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899055f-588d-4811-a2dc-bdf60127918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections = restaurants.merge(latest_inspection, how=\"left\", validate=\"1:1\", left_on=\"HSISID\",\n",
    "                                                right_index=True, indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50920144-18dc-499d-ba24-74a4dd6d0a55",
   "metadata": {},
   "source": [
    "### Investigating the join\n",
    "\n",
    "We can see that the new data frame has columns from both original data frames. The merge indicator is stored in a new column `_merge`. If contains the values \"left_only\" or \"both\" to indicate whether a particular key was only in the left dataset, or in both. It would also contain \"right_only\" if we'd done an outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8671e-23c3-4881-9fb3-72b7b6960d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6d304-6ad9-444f-94c4-f4bc502dfd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections._merge.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8c6a6-8bef-4b45-829f-e34195df0286",
   "metadata": {},
   "source": [
    "Most of the restaurants were in both datasets, with a few not having inspection records. We can proceed for now; we'll talk about handling the missing values for those in a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22823c1b-ef47-49a1-b4d8-e9056a5d10be",
   "metadata": {},
   "source": [
    "## Removing columns\n",
    "\n",
    "We don't need the `_merge` column any more, so we can drop it. We use the `.drop()` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a84ed-3543-461a-9477-958f9df370e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections = restaurant_inspections.drop(columns=[\"_merge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fb02b-f70b-4c2c-ad2e-43b990743b2c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Drop other columns we won't need: phone number and state (they're all in North Carolina)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7916f-baae-4e8b-aaf1-17acac1f948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections = restaurant_inspections.drop(columns=[\"PHONENUMBER\", \"STATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b581b5c-e858-4554-8647-1d9508b36720",
   "metadata": {},
   "source": [
    "## Doing analysis with the combined dataset\n",
    "\n",
    "We can now treat this dataset like any other, and use it in analysis.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Compute the median score by city, to see if there are any variations in sanitation between cities (perhaps due to different business licensing regulations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fd6de-3525-4bab-a409-368f138f93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"CITY\").SCORE.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a351ef-624c-4d82-b7bc-f924ef5aac54",
   "metadata": {},
   "source": [
    "## Recoding data\n",
    "\n",
    "Clearly, there are some data issues here, with several cities appearing multiple times with slightly different spellings. We'll use a few string manipulation tools Pandas gives us to try to clean up this data.\n",
    "\n",
    "Converting names to all caps will help a lot at removing duplicates. Additionally, some of the names do not match even though it looks like they should. This is often due to extra whitespace at the end. We can see if this is the issue by using `.unique()`, which will print out the values with quotes around them to see if there is extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530bb4e-05af-4a85-a96b-b9206859bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.CITY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c408e-108c-496f-a1fa-4954f2a61904",
   "metadata": {},
   "source": [
    "This does appear to be a big part of the issue. We can use the `.str` accessor to access functions for transforming strings. Here, we will use it to convert to upper case, and then strip whitespace from the start and end. Notice that these are chained together - each one returns a series, which has a `.str` accessor of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d8ca0-aa99-4b88-94a0-fa1d1cf37bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections[\"clean_city\"] = restaurant_inspections.CITY.str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d9931-0b64-456a-bf9e-91b65f8c39db",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Re-compute the median score, using the `clean_city` column. Are there still any remaining problems with the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102cc53-e390-4671-bebc-fc2d7bffbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"clean_city\").SCORE.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a215ac-ffce-4ea2-aec6-d80063e1ed5a",
   "metadata": {},
   "source": [
    "There are a few remaining issues, which we can fix manually. Morrisville and Holly Springs appear twice due to typos, and Research Triangle Park and Fuquay-Varina have two different spelling. We can use the `.replace` function to replace values with alternate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548c099-fa6e-4448-9a7d-07076dc6ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections[\"clean_city\"] = restaurant_inspections.clean_city.replace({\n",
    "    \"HOLLY SPRING\": \"HOLLY SPRINGS\",\n",
    "    \"MORRISVILE\": \"MORRISVILLE\",\n",
    "    \"FUQUAY VARINA\": \"FUQUAY-VARINA\",\n",
    "    \"RTP\": \"RESEARCH TRIANGLE PARK\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a6091-f66d-4d6c-99ae-6e3a9ea87414",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Once again recompute the median scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6aca58-9394-4ebe-95a2-6292ec7d3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"clean_city\").SCORE.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639ba38-3f7e-441a-ac45-65b7a24edd55",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "Missing data is a constant problem when working with real-world datasets. In Pandas, missing data is generally represented as `np.nan` (not a number) or `None` (a special Python data representing nothing; similar to NULL in other languages). Unlike some languages, many operations in Python will silently ignore missing data (for instance, the median above). We know there is some missing data in this dataset, because some restaurants did not have matching inspections. We can use the `isnull()` function to count how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f109e75-c796-4fa8-9dd5-02dc59cd2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.SCORE.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d3b35-0777-45d4-946f-d3d2b9f5ab27",
   "metadata": {},
   "source": [
    "That's not very many. Let's look at where they are.\n",
    "\n",
    "This is introducting a new syntax: apply with groupby, which allows performing arbitrary operations on each group. `lambda x:` creates an _anonymous function_  of x(just like the functions we've been using so far, but without a name), and whatever comes after it will be executed for each group. `x` will contain the scores for each group.\n",
    "\n",
    "If there are a lot of groups, this can be slow, but that's not an issue here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92531278-303f-4acd-8927-315c359d7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"clean_city\").SCORE.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6a438-99b9-4979-98f9-f882ea3784a8",
   "metadata": {},
   "source": [
    "There are a lot of missing values in Raleigh, but there are also a lot of restaurants in Raleigh. We can replace `.sum()` with `.mean()` to get a proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb42149-cfb5-4cc4-873a-1e5021891146",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"clean_city\").SCORE.apply(lambda x: x.isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79ee72-42c3-434b-84a2-a00543cd4b85",
   "metadata": {},
   "source": [
    "## Dropping missing data\n",
    "\n",
    "A relatively small number of data points are missing in any city. We can just drop (delete) the rows with missing data, using the `.dropna()` function. We use the subset argument to only drop rows with a missing score. We don't want to drop restaurants with a missing address line 2, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5749c0-cd16-42ac-bc5e-e836a9403539",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections = restaurant_inspections.dropna(subset=[\"SCORE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb075e-f223-46ca-8ffc-4432b47731c8",
   "metadata": {},
   "source": [
    "And we can then re-run the analysis to confirm there are no more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd113ba-6b29-454a-9278-195a27d9a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.SCORE.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b4f8a-ba8e-4a7c-bae0-a56d5205c2d2",
   "metadata": {},
   "source": [
    "## Pitfalls of filtered data\n",
    "\n",
    "When you filter data, e.g. using dropna or the filtering methods discussed in exercise 1, the original index values are preserved even after filtering. For a data frame that was indexed by a sequential row number, this can be quite confusing, as there are now numbers missing. For instance, there is no longer a row with index value 32. This isn't actually a problem, and can be useful in some cases, but it can also be quite confusing. You can use `.reset_index()` to get back to a sequential integer index, if you like. `drop=True` discards the existing index, rather than converting it to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931f521-2211-4d67-97fc-4fd1ff330ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8be6a6-9e9d-4544-9042-a8690adba36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b79f3-1f82-4e64-883d-52f648d2e48c",
   "metadata": {},
   "source": [
    "## More advanced filtering and data selection\n",
    "\n",
    "The Pandas index can be quite useful, particularly when it is not just a list of numbers, but something meaningful (e.g. the business ID). We can set the index with `set_index`. We've already seen `.loc` used for filtering, but it can also be used for selecting by index. Let's set the index for restaurant inspections to the HSISID, and then look up a restaurant by ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdd15a-5a3f-4795-9282-bd7d44b60498",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections = restaurant_inspections.set_index(\"HSISID\")\n",
    "restaurant_inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2433bc9-57ba-4af3-bb83-17ad21cb5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[4092018099]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764637d-135a-405e-b188-b78aea7ac1ea",
   "metadata": {},
   "source": [
    "You can also select specific columns with `.loc`, by including them after a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad8666-195d-4e1d-b941-a6e827cc89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[4092018099, \"NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afe332-39bc-47d0-a756-b28370348932",
   "metadata": {},
   "source": [
    "## Modifying data using indexing\n",
    "\n",
    "Sometimes, you will want to modify data in ways that cannot be directly supported by `.replace` etc. In this case, you can assignment and indexing to change values. Suppose that McDonald's has implemented a corporate program to increase sanitation at their lower-performing franchises, and the program will guarantee that every McDonald's scores at least 98 on their inspections. We'll modify the data so that all McDonald's that scored below 98 have a score of 98, and see how that influences the median scores.\n",
    "\n",
    "The first step is to find all of the McDonald's restaurants in the data frame. We can use another `.str` function, `.contains()`, to search for text in a string. `.contains` uses [regular expressions](https://docs.python.org/3/library/re.html#regular-expression-syntax). Some special characters have special meanings. For now, we'll just use it to search for the text \"McDonald\" in the dataset - with no special characters. Since there may be differences in capitalization, we convert the names to uppercase before searching. Whenever searching for a string, I like to first display the matched values to ensure I haven't over-selected (e.g. selecting \"Ronald McDonald House Cafeteria\" or something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e7336-da84-4556-9e4d-c2a8c796cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[restaurant_inspections.NAME.str.upper().str.contains(\"MCDONALD\"), \"NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb60f72-3295-4c6f-acc0-c4ac65e6a8d9",
   "metadata": {},
   "source": [
    "These all look like actual McDonald's. Next, we can filter by score, and assign a higher score to those that need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875902a0-fb2b-43fc-b7bb-f84224aa09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[\n",
    "    # we don't need parentheses around this first condition, because it's a single function call, and doesn't use any\n",
    "    # operators such as ==. If that's hard to remember, you can just put parentheses around each condition - it won't hurt\n",
    "    # anything to add the extra parentheses\n",
    "    restaurant_inspections.NAME.str.upper().str.contains(\"MCDONALD\") &\n",
    "    (restaurant_inspections.SCORE < 98),\n",
    "    \"SCORE\"\n",
    "] = 98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5952aaf-e0d0-4e53-86cf-ec19ba025397",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's check our work. Use the indexing above to display the scores for all McDonald's restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d62346-ad51-4c7e-9c3a-c9d4bb34e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[restaurant_inspections.NAME.str.upper().str.contains(\"MCDONALD\"), \"SCORE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d8879-d7d3-4c5c-901d-99db973adfb3",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Re-run the per-city median. Did the results change? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73b41f-d032-49f9-a0fb-785f492ff016",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.groupby(\"clean_city\").SCORE.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3046dbc-7c17-48a1-905b-6b04e83bd530",
   "metadata": {},
   "source": [
    "### Pitfall: chained indexing\n",
    "\n",
    "When doing assignments, you need to avoid something called \"chained indexing\". This is where you use multiple indexing or data selection operations to select the rows and columns you want to assign. Run the following code, and notice you get a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ba164-2a73-45ca-9a84-81f7c7367c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[restaurant_inspections.clean_city==\"MORRISVILLE\"][\"SCORE\"]=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914b379-57ea-4f28-89c0-70724ca97cce",
   "metadata": {},
   "source": [
    "Now, inspect the data frame to see if your assignment worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4cfa0-c099-408f-85d0-15a8fcedf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[restaurant_inspections.clean_city == \"MORRISVILLE\", \"SCORE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d2213-76b3-4bf6-918a-4157f37b143b",
   "metadata": {},
   "source": [
    "It didn't work. What happened here? This seems similar enough to what was done above.\n",
    "\n",
    "What happened is that when you select data with Pandas, sometimes you get a \"view\" of the original data - i.e. you're just looking at a portion of the original data frame, but it's still the same data. Other times, you get a \"copy\" - the values are the same, but it's a different dataset, so modifying it won't affect the original data.\n",
    "\n",
    "Pandas executed the code above from left to right. First, it selected all of the restaurants in Morrisville\n",
    "\n",
    "```python\n",
    "restaurant_inspections.loc[restaurant_inspections.clean_city == \"MORRISVILLE\"]\n",
    "```\n",
    "\n",
    "This created a copy of the data. When you then assigned a value to the score column of that copy, the original dataset was not updated. This is why we got a SettingWithCopyWarning. You shouldn't rely on this warning, though; sometimes Python will suppress warnings if you have seen them frequently.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "How would you set the score for all Morrisville restaurants to 100, without using chained indexing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c5423-9be0-42dd-b63f-076dda1c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inspections.loc[restaurant_inspections.clean_city == \"MORRISVILLE\", \"SCORE\"] = 100\n",
    "\n",
    "# check our work\n",
    "restaurant_inspections.loc[restaurant_inspections.clean_city == \"MORRISVILLE\", \"SCORE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04fc4f-6898-4a39-bb8e-4ef2609244f7",
   "metadata": {},
   "source": [
    "## Wide and long format data\n",
    "\n",
    "Another common data manipulation task is converting data between \"wide\" and \"long\" format. In \"wide\" format, you will have one row per observation, with many columns, whereas in \"long\" format you will have multiple rows with fewer columns. For instance, let's compute the median inspection score for all inspections (not just the latest) by city and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a980bcf-a95f-4e8f-9504-8d1faf4b1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections[\"year\"] = inspections.DATE_.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68bfef-c73e-4d03-bd33-f409b777d16d",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We don't have a `clean_city` variable in the inspections data frame. Merge in the restaurants dataset (this is similar to what we did before, but you want to keep every inspection record), and use the same data cleaning we used before to create the `clean_city` variable. Store the results in a new data frame `by_city_and_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a1387-1f62-4a82-ae4f-9cea4d17de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections_with_city = inspections.merge(restaurants, on=\"HSISID\", how=\"left\", validate=\"m:1\", indicator=True)\n",
    "inspections_with_city._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d58853-a3d7-416d-b098-31db2b1954e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections_with_city.dropna(subset=[\"_merge\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5aded-10b5-49be-8268-d769e2b0b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections_with_city[\"clean_city\"] = (\n",
    "    inspections_with_city.CITY.str.upper().str.strip()\n",
    "    .replace({\n",
    "    \"HOLLY SPRING\": \"HOLLY SPRINGS\",\n",
    "    \"MORRISVILE\": \"MORRISVILLE\",\n",
    "    \"FUQUAY VARINA\": \"FUQUAY-VARINA\",\n",
    "    \"RTP\": \"RESEARCH TRIANGLE PARK\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c11b98-2df4-4df9-af1a-d0be3ab1b738",
   "metadata": {},
   "source": [
    "Next, we can group by city and year, and take the median score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8a9fb-d2a8-464d-b541-5dfb87786a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_city_and_year = inspections_with_city.groupby([\"clean_city\", \"year\"]).SCORE.median()\n",
    "by_city_and_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7165c09-7b2a-4f46-8ae8-4f425610b17c",
   "metadata": {},
   "source": [
    "Is this wide or long format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929736f-0a57-49ec-9daa-5ad86453dafa",
   "metadata": {},
   "source": [
    "### Converting long to wide\n",
    "\n",
    "We can convert long to wide by using the `.unstack` function. This only works when you have multi-column index (in this case, clean_city and year). The `unstack` function will turn the last level of the index into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e02dd-ef47-4bcb-a037-123dd7a3be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_city_and_year = by_city_and_year.unstack()\n",
    "by_city_and_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff97549-39a2-47c3-b6f4-51db2a836983",
   "metadata": {},
   "source": [
    "We can convert back to the original format using the `.stack` function, which converts columns into a hierarchical row index level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43eed3-f863-4222-afe1-36c1056bad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_city_and_year.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eff9c9-0fb4-4b50-ab33-c2fcf51f75fe",
   "metadata": {},
   "source": [
    "### Converting a table without a hierarchical index\n",
    "\n",
    "In the example above, we had a hierarchical index as a result of the groupby, so unstack() made it very simple to convert one index level into column names. If you don't have data indexed by the variables you need to unstack by, you can either 1. set the index, using `.set_index([col1, col2])` where `col2` is the column you want to have become the column names, or use the `.pivot` function. `pivot` takes three named arguments: index, columns, and values, which correspond with the columns to get the index labels, the column names, and the data values from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ea263-414b-44f4-bf16-813f95665576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a long table without a meaningful index\n",
    "by_city_and_year = inspections_with_city.groupby([\"clean_city\", \"year\"]).SCORE.median().reset_index()\n",
    "by_city_and_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82b744-8079-4de0-a7e4-0559fe845879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, pivot\n",
    "by_city_and_year = by_city_and_year.pivot(index=\"clean_city\", columns=\"year\", values=\"SCORE\")\n",
    "by_city_and_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4752a4-684e-41f5-8fbc-6b45d2202ca8",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compute median score by inspector and year, and present it in a table with years for columns. You can use `.unstack` or `.pivot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e4742-2e2f-4a75-b66e-29d011b5d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections_with_city.groupby([\"INSPECTOR\", \"year\"]).SCORE.median().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d5a52-dd33-4e17-bba1-6dfc86d452aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
