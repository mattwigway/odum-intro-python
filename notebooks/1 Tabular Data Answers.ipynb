{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394b8979-776a-49bc-9073-0b3c3bf1ea43",
   "metadata": {},
   "source": [
    "# Tabular data analysis\n",
    "\n",
    "A lot of data in social sciences comes in tabular form, so analyzing tabular data is a key component of any social science oriented language. In this exercise, we will learn to load tabular data into Python and perform basic analyses. We will use data from San Francisco parking garages.\n",
    "\n",
    "This file is a Jupyter \"notebook\", which can contain code, text, images, and output. You can run each code cell separately. One pitfall is that code cells can be (and, during development, often are) run out of order. This can create errors  if code cells depend on operations done later on. It can also create non-reproducible results - for instance, if one cell multiplies all values in a column by two, if that cell is run twice the values will be multiplied by four. To help avoid these issues, I always select Kernel -> Restart Kernel and Run All Cells before finalizing results. This will restart Python, remove any leftover variables, and run the entire notebook top to bottom. If this produces errors or results change, your code was dependent on the order you ran the cells in originally, and should be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59429d7-8df3-49c0-949f-0495e274277f",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "Since Python is a general-purpose language, most data analysis tasks will require a separate library. In this exercise, we'll be using the `pandas` library for tabular data access. To use a library, we must \"import\" it. In this case, we have added `as pd` on the end of our import statement so that the library will be imported with the name `pd` rather than `pandas`. This will save typing when we have to refer to the library later in our code, and is a near-universal convention in scientific Python programming.\n",
    "\n",
    "Run the cell below by clicking in it and pressing the \"play\" button at the top of the screen, or by pressing Shift-Enter. A `[*]` will appear next to the cell to indicate that it is executing, and then this will be replaced by `[1]` when it is done to indicate the order the cells were run in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd57e5-b5d5-4618-a298-7e9ce82fbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ee285-493e-4b95-b559-ffc646633ebe",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "Next, we need to read our data. Like many datasets, the San Francisco parking garage data is distributed as a CSV file. We use the `read_csv` function from Pandas to read it. Unlike some other languages, Python requires you to spell out the library name to indicate you want to use a function from itâ€”this is why imported pandas as `pd` so we don't have to keep typing out `pandas`.\n",
    "\n",
    "We assign the result of the operation to a variable `data`. We can refer to the dataset by this name later. There's nothing special about the name `data`, we could have assigned any name consisting of alphnumeric characters, underscores, and numbers.\n",
    "\n",
    "Variables can hold any type of data - numbers, strings, or complex data structures such as tables. Unlike in some languages (e.g. Stata), variables don't refer to columns within a loaded dataset; the `data` variable contains the entire dataset.\n",
    "\n",
    "We want to read the file \"sfpark.csv\" in the \"data\" directory. Since this notebook is in the \"notebooks\" directory, we refer to the file as \"../data/sfpark.csv\" - \"..\" means the directory above this one, \"/data\" is the data directory within that, and \"/sfpark.csv\" is the CSV file within that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3879f-b035-4289-88ed-4d9bc39a5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/mattwigway/odum-intro-python/main/data/sfpark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba612acc-f9ef-4997-af66-4efb3432159e",
   "metadata": {},
   "source": [
    "## Including outputs in the notebook\n",
    "\n",
    "If a cell produces output, JupyterLab will display it inline. In the next cell, we just output the _data frame_ `data` that we just loaded, to see a preview of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bf44a-722e-4581-a665-452a37d76f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8af49-83cb-40d9-b263-1583439e2314",
   "metadata": {},
   "source": [
    "## Basic data exploration\n",
    "\n",
    "Pandas provides many basic analytical functions for columns, which can be accessed using `data.column.function()`. `data` refers to the data frame, `column` is the column name, and `function` is the function we want to execute (e.g. mean). The parentheses indicate to Python that `function` is executable code, and you want to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab592c16-48dd-4787-bed3-c53077a684be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.facility.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa99394-906b-40ea-b2ad-e051794aba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db92731-b062-44c0-974f-b67469117cb1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Compute the mean of the `exits` column\n",
    "- Since the mean may be skewed by outliers, compute the median of the entries and exits columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420e9d0-ac33-4462-bdfe-3041bdf9e074",
   "metadata": {},
   "source": [
    "## Accessing columns with special characters\n",
    "\n",
    "Columns can only be accessed using the `data.column` syntax if the column name doesn't contain any special characters, and don't start with a number. We can use an alternate syntax to access colums with special characters. Here we also see the use of the `value_counts` function, which provides a count of the number of times each unique value occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd030f-3090-4f79-ad93-e369d8614e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Usage Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1273d84-8628-4fdb-ae3c-4b1507a9bc79",
   "metadata": {},
   "source": [
    "## Renaming columns\n",
    "\n",
    "I prefer to use the `data.column` syntax for accessing columns. We can rename the columns using the `data.rename()` function.\n",
    "\n",
    "This code introduces a few new concepts. `pandas` provides not only functions based on columns, but also functions based on the entire dataset. Functions can take _arguments_, which are values provided when we call the function. In this case, we specify the column names, and that we want the operation to occur \"in-place\" - i.e. by modify `data`, rather than creating a new data frame with the column name changes.\n",
    "\n",
    "Lastly, the argument to `columns=` is a _dict_, short for dictionary. A dict contains keys (on the left side of the `:`) and values (on the right side). Values can be looked up based on the corresponding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa6537-c9c9-43df-bfe9-c9f3d68470a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"Usage Type\": \"usage_type\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8a06e-3b36-46fb-89f5-8eacb94a7cce",
   "metadata": {},
   "source": [
    "## Grouped data analysis, also known as split-apply-combine\n",
    "\n",
    "Grouped data analysis is a very common pattern - rather than a mean over the entire dataset, we may want a mean by groups. For instance, the median being so different from the mean suggests outliers - perhaps one very large garage. Let's look at the mean entries by garage.\n",
    "\n",
    "We do this by grouping the dataset by the unique values of the `facility` column, and then taking the mean of the entries column. We will get a single mean for each group. We can see that there are significant outliers, with several averages above 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c840f-2d4e-43ce-a9fa-391608a4f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"facility\").entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ada70d-9df5-43f6-aa48-4329a9c17fd9",
   "metadata": {},
   "source": [
    "## Understanding the dataset\n",
    "\n",
    "We have taken the mean number of entries across all the records for each garage. Is this the mean daily entries in each garage? Let's look at the dataset again. Does each row represent a full day of usage for a garage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809c215-4cb7-410a-ba2c-9d9a9fa28cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5139d37-abbb-4126-9a1c-fefaae615e1d",
   "metadata": {},
   "source": [
    "## Creating a daily dataset\n",
    "\n",
    "We can group by garage and day, and sum entries. To group by multiple columns, we enclose the column names in `[]`, which creates a _list_ or array.\n",
    "\n",
    "You can have multiple lines of code in a single code cell. In this case, only the output of the last line will be displayed. Here, we perform grouping and then display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0d6f7-dd2a-4181-b7b2-88fa1c57a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date = data.groupby([\"facility\", \"date\"]).entries.sum()\n",
    "by_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53175428-fa86-4be0-8d3c-5a21fc820cd1",
   "metadata": {},
   "source": [
    "## Pandas data types\n",
    "\n",
    "We have already worked with data frames, which contain tabular data. You may notice that printout above looks different from the data frame printouts we've seen earlier. That is because the group by sum has returned a _series_. A series is just a vector of values of the same type (integers in this case), with an _index_. An index is a set of labels for the items. Tables also have an index. Our data table just has row numbers as its index. However, any result from groupby is indexed by the column(s) used to group. If there are multiple columns, the data is indexed first by the one specified first in `groupby`, and so on.\n",
    "\n",
    "We can easily access rows by index value using the `.loc` attribute of a series or data frame. We enclose the two values we want to look up in `()`, which creates a _tuple_. This is basically the same as a list, except for that it cannot be modified once created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02247a00-20d8-462b-b0c6-a39699d573f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date.loc[(\"16th and Hoff Garage\", \"1/1/2012\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d5bba-64a2-409b-9281-3ce9a4e6aeb5",
   "metadata": {},
   "source": [
    "## Converting indices back to regular columns\n",
    "\n",
    "We can use the `reset_index` function to convert the series back to a regular data frame, with columns and an index based on row numbers, and assign the result back to the variable `by_date`, overwriting the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92921edd-8a26-467e-93af-8b39e7b8decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date = by_date.reset_index()\n",
    "by_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a0868-bd45-40be-afdb-d6b642eafbd8",
   "metadata": {},
   "source": [
    "## Exercise: per garage averages\n",
    "\n",
    "Now that we have a single record for each garage and each day, compute the average daily entries by garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75104c8-d411-4a61-a5e6-a60b7ffab127",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date.groupby(\"facility\").entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127eed0-576f-4d5f-aab9-6ae2a321b024",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "Sometimes we don't want to work with the entire dataset. Let's extract just the data from garages in the Mission district. We can do this using _boolean indexing_, where we use a create an array of boolean (true/false) values used to select data, and use the `.loc` accessor to extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b63f8f-766a-4c92-810b-a331e520252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_data = data.loc[data.district == \"Mission\"]\n",
    "mission_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef009d-5fd4-4c4a-b341-cfd5f45d0d78",
   "metadata": {},
   "source": [
    "## Digging into filtering a bit more\n",
    "\n",
    "What the above code does is create an array of booleans, one for each row, with a true value if that row is from a garage in the Mission. We can look at this array directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960359c-946f-453a-9633-526a15d56b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_bool = data.district == \"Mission\"\n",
    "mission_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f4570-c379-4405-afc8-6bf81371cf85",
   "metadata": {},
   "source": [
    "When that array is used in `.loc`, it selects all rows where the corresponding value is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87161f5a-95d0-41d3-875d-01fa62ce2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[mission_bool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecac66-7b9a-4b9e-99a6-727f322c0847",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Compute daily per-garage averages using just the data from the Mission District."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c786d0e-ab39-4837-a7cd-7bf27abf2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_by_date = mission_data.groupby([\"facility\", \"date\"]).entries.sum().reset_index()\n",
    "mission_by_date.groupby(\"facility\").entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc7e23-7f0e-4c26-9677-19dc5cd27eed",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "Every column in an Pandas data frame has a data type. These could be integers (usually represented as `int64` for a 64-bit integer), floating-point numbers (generally `float64`), objects (any Python object, but usually used for strings), dates, etc. We can see the data types in the `.dtype` field of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c2449-a689-4965-bd0f-e325526a6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663745-1ae3-43d3-a7f4-01df4b58b46a",
   "metadata": {},
   "source": [
    "## Categorical data\n",
    "\n",
    "Oftentimes, string/object columns will have just a few unique values. In these cases, it is often valuable to convert these columns to a categorical data type. From a user perspective these behave almost the same as string columns, but can be stored and manipulated more efficiently. With large data sets, this translates to more efficient data analysis and lower memory usage. In this dataset, usage_type, facility, and district can all be considered categorical.\n",
    "\n",
    "We can convert the relevant columns with `.astype(\"category\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be729e5a-7c93-4aed-8c82-57c9170a4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"usage_type\"] = data.usage_type.astype(\"category\")\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd50bea-fd71-44ff-b50f-9e3463b01098",
   "metadata": {},
   "source": [
    "### Exercise: convert the facility and district columns as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dbfb9-3f3b-4504-9147-7dbe9e69ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"facility\"] = data.facility.astype(\"category\")\n",
    "data[\"district\"] = data.district.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227fcdf-573e-4d7b-a9ba-b38c53f502fa",
   "metadata": {},
   "source": [
    "### Categorical data accessors\n",
    "\n",
    "There are a number of functions for working with categorical data available within the \"category accessor\", `data.column.cat`. For example, we can display the categories in a categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffe095-ca7b-48ba-b92a-cd66649985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.usage_type.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10727e1b-5b7e-4f99-9c61-e6e4db67ae1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with dates\n",
    "\n",
    "We see that the date is represented as an `object`, not a date. The date has been read as a string rather than parsed as a date. We can use the `pd.to_datetime` function to parse dates. Specifying a date format is optional, but I always like to do it to ensure that the dates are parsed correctly. Date formats are specified using [the codes found here](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes). I like to run `pd.to_datetime` once to make sure it parsed correctly before assigning it to a column in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696e215-cac5-4a7e-87d8-f4713d1279a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(data.date, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab543c-0bbd-4d8b-9b9d-1eaa9565ac42",
   "metadata": {},
   "source": [
    "Now, we can assign it to a column in the data frame. Creating new columns or overwriting old columns requires using the `dataframe[\"column name\"]` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58e5da-489c-42b5-8542-a5965aebc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data.date, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e1ca8-874d-4801-b4df-b09a1ed1835e",
   "metadata": {},
   "source": [
    "Now, we can ensure that data types are correct. Notice that the data type for the `date` column is no longer `object`, but `datetime64[ns]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a028e37-fa50-4fac-aa09-409551bbcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0ef97-ad0f-4df6-8cbf-9367dc31239a",
   "metadata": {},
   "source": [
    "### Filtering by date\n",
    "\n",
    "Standard mathematical operations work with date types; for instance, we can extract all records between January 15 and February 15, 2012.\n",
    "\n",
    "Note that we are filtering by two conditions here, and putting them together with an `&`, meaning and. In Python, unlike many other languages, you need to put parentheses around each condition due to order of operations. Otherwise, the \"and\" operation will take place before the comparison operations, leading to errors or incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f091a-d94d-4253-b337-fb1181b24d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jf_2012 = data.loc[\n",
    "    (data.date <= pd.to_datetime(\"2/15/2012\", format=\"%m/%d/%Y\")) &\n",
    "    (data.date >= pd.to_datetime(\"1/15/2012\", format=\"%m/%d/%Y\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42800cfd-3ccf-4bc4-bf9e-19eb388dd254",
   "metadata": {},
   "source": [
    "#### Exercise: compute mean entries per day by garage in January/February 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e004f-84e8-4798-9a37-3b02c380ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date_jf_2012 = data_jf_2012.groupby([\"facility\", \"date\"]).entries.sum().reset_index()\n",
    "by_date_jf_2012.groupby(\"facility\").entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57289c-39df-4dd7-acea-556e8bbc94b0",
   "metadata": {},
   "source": [
    "### Operations with dates\n",
    "\n",
    "There are a number of date functions available with the `.dt` DateTime accessor. For instance, we can filter to only weekend days using the `day_name` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b52760-bcf3-4313-8d31-ea6b28965d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekends = data.loc[data.date.dt.day_name().isin([\"Saturday\", \"Sunday\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b2176-5413-491b-9349-09ded9f64109",
   "metadata": {},
   "source": [
    "#### Exercise: compute mean daily _weekday_ entries, by garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34d56f-6773-421a-b939-c3f227480f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = data.loc[data.date.dt.day_name().isin([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])]\n",
    "weekdays_by_date = weekdays.groupby([\"facility\", \"date\"]).entries.sum().reset_index()\n",
    "weekdays_by_date.groupby(\"facility\").entries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c01c9f-90c3-418f-837a-719a57fc8738",
   "metadata": {},
   "source": [
    "## Checking our work\n",
    "\n",
    "Python has an `assert` statement, which will produce an error if whatever condition comes after it is not true. For instance, we can confirm that our weekend dataset contains Saturday and Sunday records. This would error if we had, for example, misspelled Saturday above and only included Sunday records. I like to use assert statements liberally throughout my code to check that operations completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcd1b3-0e83-4c03-8b3a-28d953910064",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (weekends.date.dt.day_name().unique() == [\"Saturday\", \"Sunday\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409368b-8c04-41cb-a322-7c84fbd25752",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create two assert statements to make sure that our data_jf_2012 dataset contains only dates from January 15, 2012 to February 15, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557e2ec-1046-4be8-9b54-485cba979962",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_jf_2012.date >= pd.to_datetime(\"1/15/2012\", format=\"%m/%d/%Y\")).all()\n",
    "assert (data_jf_2012.date <= pd.to_datetime(\"2/15/2012\", format=\"%m/%d/%Y\")).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc27b3d-ed53-41d3-8b75-d3028967ce10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
